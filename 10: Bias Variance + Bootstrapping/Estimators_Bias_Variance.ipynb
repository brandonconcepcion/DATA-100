{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Estimators, Bias, and Variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18.1 Common Random Variables**\n",
    "\n",
    "We will use many random variables throughout DATA 100: \n",
    "\n",
    "#### **Bernoulli**$(p)$\n",
    "* Takes on value $1$ with probability $p$, and $0$ with probability $(1-p)$\n",
    "* Also called the *\"indicator\"* random variable\n",
    "* **Expectation:** $E[X] = p$\n",
    "* **Variance:** $\\text{Var}(X) = p\\cdot (1-p)$\n",
    "\n",
    "#### **Binomial**$(n, p)$\n",
    "* Number of $1$s in $n$ independent $\\text{Bernoulli}(p)$ trials\n",
    "* **Expectation:** $E[Y] = np$\n",
    "* **Variance:** $\\text{Var}(X) = np\\cdot (1-p)$\n",
    "\n",
    "Uniform on a finite set of values \n",
    "* The probability of each value is $\\frac{1}{\\text{(number of possible values)}}$\n",
    "* Example: a standard / fair die \n",
    "\n",
    "Uniform on the unit interval $(0,1)$\n",
    "* Density is flat at $1$ on $(0,1)$ and $0$ elsewhere \n",
    "\n",
    "Normal $(\\mu, \\sigma ^2)$ a.k.a Gaussian \n",
    "* $f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp} (\\frac{-1}{2}(\\frac{x - \\mu}{\\sigma})^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18.2 Sample Statistics**\n",
    "\n",
    "* The distribution of a *population* describe how a random variable behaves across *all* individuals of interest \n",
    "\n",
    "* The distribution of a *sample* describes how a random variable behaves in a *specific sample* from the population \n",
    "\n",
    "* In Data Science, we oftentimes don't have access to the whole population, so we make use of samples in order to make inferences \n",
    "\n",
    "* When sampling, we also make the **BIG** assumption that we sample uniformly at random *with replacement* from the population \n",
    "\n",
    "* Our sample mean is a random variable, as it depends on our randomly drawn sample \n",
    "\n",
    "* The population mean, on the other hand, is a **fixed** number\n",
    "\n",
    "\n",
    "### **18.2.1 Sample Mean**\n",
    "\n",
    "We define the sample mean as \n",
    "\n",
    "$$ \\bar{X_n} = \\frac{1}{n} \\sum_{i = 1}^n X_i$$\n",
    "\n",
    "The expectation of the sample mean as \n",
    "\n",
    "$$ E[\\bar{X_n}] = \\mu $$\n",
    "\n",
    "The variance of the sample mean as \n",
    "\n",
    "$$\\text{Var}(\\bar{X_n}) = \\frac{\\sigma ^2}{n}$$\n",
    "\n",
    "### **18.2.2 Central Limit Theorem**\n",
    "* If an independent and identically distributed sample of $n$ is large, then the probability distribution of the **sample mean** is **roughly normal** with mean $\\mu$ and SD of $\\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/clt.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "\n",
    "### **18.2.3 Using the Sample Mean to Estimate the Population Mean**\n",
    "* If we want to use our sample mean to estimate our true population mean, one sample is not enough (what if we get a bad sample?)\n",
    "* What if our sample size is also too small?\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/CLTdiff.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "\n",
    "* We can see that there is less variability in a sample size of $800$\n",
    "\n",
    "These are all questions which are remedied by **bootstrapping**, which will be covered in the next note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18.3 Prediction and Inference**\n",
    "\n",
    "**Inference**: The task of using a model to infer the true underlying relationships between the features and response variable \n",
    "\n",
    "* One major goal of inference is to draw conclusions about the full population of data by using only one random sample \n",
    "\n",
    "We have two important definitions: \n",
    "* **parameter**: a numerical function of the *population* (like $\\mu$)\n",
    "* **statistic** A numerical function of the random *sample* (like $\\bar{X_n}$) \n",
    "\n",
    "Due to it's random nature, we call the statistic an **estimator** of the true population parameter\n",
    "* Let $\\theta$ denote the population parameter \n",
    "* $\\hat{\\theta}$ denotes the estimator\n",
    "\n",
    "Evaluating a good estimator using the following metrics: \n",
    "* How close is our answer to the parameter **(Risk / MSE)**\n",
    "\n",
    "$$\\text{MSE}(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)]^2 $$\n",
    "\n",
    "* Do we get the right answer for the parameter, on average? **(Bias)**\n",
    "$$\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$$\n",
    "\n",
    "* How variable is the answer? **(Variance)**\n",
    "\n",
    "$$\\text{Var}(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\theta])^2] $$\n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/bias_v_variance.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "In an ideal world, we want our estimator to have low bias and low variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **18.3.1 Prediction and Inference**\n",
    "\n",
    "* Let's consider the relationship $Y = g(x)$ where $g$ represents some \"universal truth\" that defines the underlying relationship between $x$ and $Y$\n",
    "\n",
    "* We have plotted $g$ with the red line below\n",
    "\n",
    "* As Data Scientists, we actually never get to see $g$\n",
    "\n",
    "* When we collect data in order to try and estimate $g$, our process will always involve some inherent error\n",
    "\n",
    "* As a result, we say that each observation comes with a **noise**, or random error term $\\epsilon$\n",
    "\n",
    "* We say that $\\epsilon$ is a random variale with $E[\\epsilon] = 0$ and $\\text{Var}(\\epsilon = \\sigma ^ 2)$, thus making $Y(X)$ also a random variable \n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/data.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "We construct the model $\\hat{Y}(x)$ to estimate $g$\n",
    "\n",
    "* $\\text{True relationship: } g(x)$\n",
    "* $\\text{Observed relationship: }Y = g(x) + \\epsilon$\n",
    "* $\\text{Prediction: }\\hat{Y}(x)$\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/y_hat.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "Choice of features also significantly impact our estimation\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/y_hat2.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18.4 Bias-Variance Tradeoff** ##\n",
    "\n",
    "With our model: \n",
    "* $\\text{True relationship: } g(x)$\n",
    "* $\\text{Observed relationship: }Y = g(x) + \\epsilon$\n",
    "* $\\text{Prediction: }\\hat{Y}(x)$\n",
    "\n",
    "We can revisit the bias-variance tradeoff curve: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/bvt_old.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "and get something that looks more like this: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/bvt.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **18.4.1 Model Risk**\n",
    "\n",
    "* **Model risk** is defined as the mean square prediction error of the random variable $\\hat{Y}$\n",
    "\n",
    "* It's an expectation across *all* samples we could have possiblly gotten when fitting the model \n",
    "\n",
    "* Considers the model's performance on any sample that is theoretically possible, rather than the specific data we have collected \n",
    "\n",
    "$$\\text{model risk }=E\\left[(Y-\\hat{Y(x)})^2\\right]$$\n",
    "\n",
    "The origin of model risk comes from as follows: \n",
    "1) **Observation Variance**: Randomness in new observations $Y$ due to random noise $\\epsilon$\n",
    "2) **Model Variance**: Randomness in the sample we used to train the models, as samples $X_1, X_2, \\ldots, X_n, Y$ are random \n",
    "3) **Model Bias**: non-random error due to our model being different from the true underlying function $g$ \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/errors.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "Let's now zoom in on a single point: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/error.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:500px; height=550px;\">\n",
    "\n",
    "* Note that $\\hat{Y}(x)$ is a random variable - it's prediction for $x$ depends on the specific sample used for training\n",
    "\n",
    "We can identify three components of error from the graph above: \n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/decomposition.png\n",
    "\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:700px; height=750px;\">\n",
    "\n",
    "Putting all these errors into one equation, we get the following decomposition of model risk: \n",
    "\n",
    "$$E\\left[(Y(x)-\\hat{Y}(x))^2\\right] = E[\\epsilon^2] + \\left(g(x)-E\\left[\\hat{Y}(x)\\right]\\right)^2 + E\\left[\\left(E\\left[\\hat{Y}(x)\\right] - \\hat{Y}(x)\\right)^2\\right]$$\n",
    "\n",
    "Let's now go term by term and see if we can simplify specific terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **18.4.1.1 Observation Variance**\n",
    "\n",
    "The first term in the above decomposition is $E[\\epsilon^2]$\n",
    "* Recall $\\mathbb{E}(\\epsilon)=0$ and $\\text{Var}(\\epsilon) = \\sigma^2$. \n",
    "* As a result, \n",
    "\n",
    "$$ \\text{Observation variance} = \\text{Var}(\\epsilon) = \\sigma^2$$\n",
    "\n",
    "* **observation variance** exists due to randomness in our observations of $Y$\n",
    "* It is a form of chance error\n",
    "\n",
    "#### **18.4.1.2 Model Variance**\n",
    "* We now look at the last term: $E\\left[\\left(E\\left[\\hat{Y}(x)\\right] - \\hat{Y}(x)\\right)^2\\right]$\n",
    "\n",
    "* This is precisely $\\text{Var}(\\hat{Y}(x))$, or **model variance**\n",
    "\n",
    "* Describe how much $\\hat{Y}(x)$ tends to vary when we fit the model on different samples \n",
    "\n",
    "* Describe the variability due to the randomness in our sampling process\n",
    "\n",
    "* Also a form of *chance error* \n",
    "\n",
    "$$\\text{model variance} = \\text{Var}(\\hat{Y}(x)) = E\\left[\\left(\\hat{Y}(x) - E\\left[\\hat{Y}(x)\\right]\\right)^2\\right]$$\n",
    "\n",
    "* Large model variance is often a result of **overfitting**, where we pay too much attention to the small differences in our sample which lead to large differences in the fitted model \n",
    "\n",
    "* In order to remedy this **overfitting**, we can reduce model complexity (take out some features)\n",
    "\n",
    "#### **18.4.1.3 Model Bias**\n",
    "\n",
    "* The second term is $\\left(g(x)-E\\left[\\hat{Y}(x)\\right]\\right)^2$, and refers to **model bias**\n",
    "\n",
    "* **Model Bias** is how far off $g(x)$ and $\\hat{Y}(x)$ are on average over all possible samples \n",
    "\n",
    "$$\\text{model bias} = E\\left[\\hat{Y}(x) - g(x)\\right] = E\\left[\\hat{Y}(x)\\right] - g(x)$$\n",
    "\n",
    "* Model bias is not random; it's an average measure for a specific individual $x$ \n",
    "\n",
    "* If bias is positive, our model tends to overestimate $g(x)$\n",
    "\n",
    "* If bias is negative, our model tends to underestimate $g(x)$\n",
    "\n",
    "* If it's $0$, we can say that our model is **unbiased**\n",
    "\n",
    "There are two main reasons for large model biases \n",
    "* Underfitting: our model is too simple \n",
    "* Bad Domain knowledge: We don't understand what features are useful for the response \n",
    "\n",
    "We can remedy this by making our model more *complex* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **18.4.2 The Decomposition**\n",
    "\n",
    "With this in mind, we can update our decomposition of model risk \n",
    "\n",
    "$$E[(Y(x) - \\hat{Y}(x))^2] = \\sigma^2 + (E[\\hat{Y}(x)] - g(x))^2 + \\text{Var}(\\hat{Y}(x))$$\n",
    "\n",
    "$$\\text{model risk } = \\text{observation variance} + (\\text{model bias})^2 \\text{+ model variance}$$\n",
    "\n",
    "This is known as **bias-variance tradeoff**\n",
    "* Reducing Model Bias to increasing Model Variance (increase complexity)\n",
    "* Decreasing Model Variance by increasing Model Bias (decreasing complexity)\n",
    "\n",
    "Remember, \n",
    "* High Variance and Low Bias (**overfitting**)\n",
    "* Low Variance and High Bias (**underfitting**)\n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/probability_2/images/bvt.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:700px; height=750px;\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
