{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **19: Causal Inference and Confounding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **19.1 Parameter Inference: Interpreting Regression Coefficients**\n",
    "\n",
    "There are two main reasons why we build models \n",
    "1) **Prediction:** using our model to make accurate predictions about unseen data \n",
    "2) **Inference:** using our model to draw conclusions about the underlying relationships between our feature and response. \n",
    "\n",
    "In the model we've created to estimate $g(x)$ (the true underlying relationship between two variables), we came up with the following equation\n",
    "\n",
    "$$f_{\\hat{\\theta}}(x) = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_1 + \\ldots + \\hat{\\theta}_p x_p$$\n",
    "\n",
    "* Note that we assume that the relationship we're trying to model is indeed linear \n",
    "\n",
    "But what does each coefficient $\\theta_i$  actually *mean?*\n",
    "* Large values of $\\theta_i$ mean that feature $x_i$ has a large effect on the response \n",
    "* Small values of $\\theta_i$ mean that feature $x_i$ has a smaller effect on the response \n",
    "* If $\\theta_i$ is $0$, then feature $x_i$ has **no effect** on the response \n",
    "\n",
    "So, if the *true* value of $\\theta_i$ is indeed $0$, we can conclude that there is no underlying relationship between $x_i$ and $Y(X)$\n",
    "\n",
    "How do we test if $\\theta_i$ is really $0$, we can utlilize a **hypothesis test**\n",
    "* Our null is that $\\theta_i$ is $0$\n",
    "* Our alternative is that $\\theta_i$ is *not* $0$\n",
    "* If the p-values for certain features are less than cutoff (usually $0.05$), we *reject* the null in favor of the alternative \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **19.2 Review: Bootstrap Resampling**\n",
    "\n",
    "* We make use of the bootstrap when we want to determine the properties (e.g. variance) of the sampling distribution of an estimator\n",
    "\n",
    "* In many cases, we *only* have one sample from our population \n",
    "\n",
    "* The remedy for this is to treat our *original* sample as if it was the population, and then randomly *resample* with replacement from that sample\n",
    "\n",
    "* As a result, bootstrap resamples will simulate *as if* we are sampling from our original population again \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/inference_causality/images/bootstrap.png\" alt=\"Line Chart\" style=\"display:inline-block; margin-right:10px; width:700px; height=750px;\">\n",
    "\n",
    "* Some things to keep in mind: \n",
    "    * We need to sample the same way we constructed the original sample, most of the time by taking a simple random sample *with replacement* \n",
    "\n",
    "    * New samples must also be the same size as the original sample, in order to accurately model the variability of our estimates\n",
    "\n",
    "* The center and spread of bootstrapped distribution are *approximations*\n",
    "\n",
    "* If our original sample (which we take bootstraps of) is not representative of the true population, then the boostrap is not very useful \n",
    "\n",
    "* In general, bootstrapping works better for *large samples*, when the population distribution is *not heavily skewed* (no outliers), and when the estimator is \"*low variance*\" (insensitive to extreme values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **19.3 Collinearity**\n",
    "\n",
    "**Collinearity**, also sometimes called **multicollinearity**, occurs when one feature can be predicted fairly accurately by a linear combination of other features\n",
    "* Happens when one feature is highly correlated with the others \n",
    "\n",
    "Why is collinearity an issue?\n",
    "* **Inference**: Slopes can't be interpreted for an inference task \n",
    "\n",
    "* **Model Variance**: If features strongly influence one another, even small changes in the sampled data can lead to large changes in the estimated slopes \n",
    "\n",
    "* **Unique Solution**: If one feature is a linear combination of other features, the designed matrix will not be full rank, and $X^{\\text{T}}X$ is not invertible\n",
    "\n",
    "The main point is as follows: \n",
    "* We need to be careful with what features we select for modeling. If two featrues encode similar information, it is often a good idea to choose only one of them as an input variable \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
