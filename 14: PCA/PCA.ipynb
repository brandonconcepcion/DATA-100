{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is an **unsupervised learning technique** - our goal is to identify patterns in data, rather than predict outputs \n",
    "* We use *unlabeled* data\n",
    "* Since we have features and no labels, we want to instead identify patterns within those features\n",
    "\n",
    "Specifically: Given a high-dimensional dataset (many features), can we find a simpler representation to summarize the main patterns of the data?\n",
    "\n",
    "### PCA performs **Dimensionality Reduction**\n",
    "* Transform high dimensional data down to low dimensions \n",
    "* Re-express the data in terms of fewer features, without losing (much) info \n",
    "\n",
    "There are two equivalent ways of framing PCA: \n",
    "1) Finding directions od **maximum variability** in the data \n",
    "2) Finding the low dimensional (rank) matrix factorization that **best approximates the data**\n",
    "\n",
    "The first approach limits us to work with attributes individually\n",
    "* It cannot resolve collinearity, and we cannot combine features as a result \n",
    "\n",
    "The second approach uses PCA to construct **principal components** with the most variance in the data by utilizing **linear combinations of features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Procedure**\n",
    "\n",
    "To perform PCA on a matrix \n",
    "1) **Center** the data matrix by subtracting the mean of each attribute column \n",
    "2) Use Singular Value Decomposition (SVD) to find all principal components efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Singular Value Decomposition**\n",
    "\n",
    "To make a long story short, SVD describes a matrix $X$'s decomposition into three matrices: \n",
    "\n",
    "$$X = USV^T$$\n",
    "\n",
    "Let's break down each of these terms one by one \n",
    "\n",
    "#### $U$ \n",
    "* $U$ can be thought of as the \"rotation\" matrix \n",
    "* The columns are **orthonormal**\n",
    "\n",
    "#### $S$ \n",
    "* $S$ can be though of as the scaling operation \n",
    "* Contains $r$ **non-zero singular values**, where $r$ is the rank of $X$ \n",
    "* Diagonal values (singular values $s_1$, $s_2$, ... $s_r$) are **non-negative** ordered from largest to smallest $s_1 \\ge s_2 \\ge ... \\ge s_r > 0$\n",
    "\n",
    "#### $V^T$\n",
    "* When transposed, contains the principal components \n",
    "* So, the first $k$ columns of $V$ (different from $V^T$) contain the first $k$ principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capturing Variance** \n",
    "\n",
    "* To preserve as much information as we can about our dataset, we want our new features to capture the variance of the original data\n",
    "\n",
    "* Say you're only allowed to use **one** linear combination of the features to represent this two-feature data - what line would you draw?\n",
    "\n",
    "\n",
    "<img src=\"PCA1.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "* You can think of this first line as the **first** Principal Component\n",
    "\n",
    "Now, where should the second line go?\n",
    "\n",
    "<img src=\"PCA1_2.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "* You can think of this second line as the **second** Principal Component\n",
    "\n",
    "Essentially, we've created a **new coordinate system** to describe our data\n",
    "\n",
    "We most often use $2$ PCs when conducting PCA \n",
    "* This is so we can easily visualize this on a scatterplot \n",
    "* PC1 goes on the $x$ axis \n",
    "* PC2 is on the $y$ axis \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spinning Around**\n",
    "\n",
    "* The lines we've drawn define a **new coordinate system** to describe our data \n",
    "* The direction that contains the most variance in our data is along one axis; the direction that contains thenext most variance is along the other axis \n",
    "* We've **rotated** the coordinate system we originally used to represent the data \n",
    "\n",
    "$$XV = US$$\n",
    "\n",
    "* $X$ represents our original data \n",
    "* $V$ rotates our data into the new coordinate system \n",
    "* $US$ gives the principal components \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dimensionality Reducton**\n",
    "* Remember that we constructed the first Principal Component to capture the most variace in the data, the second PC to capture the second most, and so on \n",
    "\n",
    "* This means that if we were to represent our dataset using only the first few PCs, we'd still be able to capture most of the variance in the data \n",
    "\n",
    "$$(US)_{[:,:n]} V^{\\text{T}}_{[:n,:]} = \\text{Rank n approximation of } X$$\n",
    "\n",
    "* Taking the first $n$ columns of $US$ is equivalent to taking the first $n$ Principal Components \n",
    "* We multiply by the first $n$ rows of our transposed rotation matrix \n",
    "* We then get an approximation of the original data byusing only $n$ dimensions \n",
    "\n",
    "This is what we call **dimensionality reduction** - we can make pretty good approximations of the original data by only using a few features\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "How do we know how many PCs to use? (What value of $n$ should we be using?)\n",
    "* $S$ is the singular value matrix = it measure how much variance each PC captures / contributes \n",
    "\n",
    "<img src=\"PCA1_3.png\" alt=\"Image Alt Text\" width=\"400\" height=\"250\">\n",
    "\n",
    "We denote the variance captured by a specific PC as: \n",
    "\n",
    "$$\\text{variance captured by } n\\text{th PC} = \\frac{(i\\text{th singular value})^2}{n}$$\n",
    "\n",
    "A **scree plot** displays the variance captured by each PC. \n",
    "* We use whatever number of PCs which captures the bulk of the variance in the data \n",
    "* Describes the **variance ratio** captured by each principal component, with the largest ration first \n",
    "* Help us determine visually the number of dimensions needed to describe the data reasonably\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/pca_2/images/scree_plot.png\" alt=\"Image Alt Text\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Biplots**\n",
    "\n",
    "* Biplots superimpose the directions onto the plot of PC1 vs. PC2\n",
    "* The vector $j$ corresponds to the diretion for feature $j$ (e.g. $v_1j$, $v_2j$)\n",
    "* In DATA 100, we plot the direction itself \n",
    "\n",
    "In the below diagram, we are able to interpret ow features correlate with the principal components shown: positively, negatively, or not much \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/pca_2/images/slide17_2.png\" alt=\"Image Alt Text\" width=\"600\" height=\"450\">\n",
    "\n",
    "* The direction of the arrows show how that feature contributes to PC1 and PC2\n",
    "* Let's consider feature 3, with the purple arrow labeled $520$\n",
    "\n",
    "* We want to break up it's components into $v_1$ and $v_2$, assume $v_1$ corresponds to the PCA $1$ axis and $v_2$ the PCA $2$ axis\n",
    "\n",
    "* Since $v_1$ is positive, then a linear increase in this feature would correspond to a linear increase in PC $1$ (This feature and PC $1$ are positively correlated)\n",
    "\n",
    "* Since $v_2$ is negative, then a linear decrease in this feature would correspond to a linear decrease in PC $2$ (This feature and PC $1$ are positively correlated) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
