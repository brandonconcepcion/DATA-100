{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Constant Model, Loss, and Transformations**\n",
    "\n",
    "### **The modeling Process** ###\n",
    "\n",
    "Modeling deals with: How can I use a set of $x$ variables in order to predict the value of a $y$ variable?\n",
    "* Can we use height to predict weight?\n",
    "* Do more hours studying lead to better grades?\n",
    "\n",
    "\n",
    "In order to predict target variables as functions of specific features, we estbalish the following workflow \n",
    "1) Choose a model - how should we represent the world?\n",
    "2) Choose a loss function - how do we quantify prediction error?\n",
    "3) Fit the model - how do we choose the best parameter of our model given our data?\n",
    "4) Evaluate model performance - how do we evaluate whether this process gave rise to a good model?\n",
    "\n",
    "\n",
    "Prediction vs Estimation: \n",
    "* **Prediction** is the task of using a model to predict outputs for unseen data\n",
    "* **Estimation** is the task of using data to calculate model parameters \n",
    "\n",
    "For example, in Simple Linear Regression we have \n",
    "\n",
    "$$\\hat{y} = \\hat{\\theta_1} + \\hat{\\theta_2}$$\n",
    "* We **estimate** the parameters by minimizing average loss (Often times through Least Squares Estimation, or minimizing MSE)\n",
    "* We **predict** using these estimations \n",
    "\n",
    "### **Evaluating the SLR Model** ###\n",
    "\n",
    "How \"good\" are the predictions made by this \"best\" fit model?\n",
    "\n",
    "1) Visualize data and compute statistics \n",
    "    * Plot the original data \n",
    "    * Compute each column's mean and standard deviation (if both of these are close to the original observed $y$ values, we might be inclined to say our model did well)\n",
    "    * Compute $r$, the correlation coefficient. A large magnitude of $r$ between features and response could also indicate our model has done well\n",
    "\n",
    "2) Performance metrics \n",
    "    * We can take the **Root Mean Squared Error (RMSE)**\n",
    "    * A lower RMSE indicates more \"accurate\" predictions, as we have lower \"average loss\" across the data \n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "3) Visualization \n",
    "    * Look at the residual plot of $e_i = y_i - \\hat{y_i}$ to visualize differences between actual and predicted values \n",
    "    * A good residual plot should not show any pattern between inputs $x$ and residual values $e_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constant Model + MSE** ###\n",
    "\n",
    "* **Constant** models are also known as a summary statistics \n",
    "* Always predicts the *same constant number* \n",
    "* Employs a **simplifying assumption**, and does not take into account the relationships between variables \n",
    "\n",
    "$$\\hat{y} = \\theta_0$$\n",
    "\n",
    "#### **What is the optimal** $\\theta_0$ **for L2 Loss (MSE)** ####\n",
    "* What number should we guess each time in order to have the lowest possible **average loss?**\n",
    "* If we use Mean Squared Error as our loss function, then with a constant model our loss function is minimize at $\\hat{\\theta_0} = \\bar{y}$\n",
    "\n",
    "#### **What is the optimal** $\\theta_0$ **for L1 Loss (MAE)?** ####\n",
    "* What number should we guess each time in order to have the lowest possible **average loss?**\n",
    "\n",
    "* If we use Mean Absolute Error as our loss function, then with a constant model our loss function is minimize at $\\hat{\\theta_0} = \\text{median}(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary: Loss Optimization, Calculus, and Critical Points** ####\n",
    "\n",
    "First, define the **objective function** as average \n",
    "* Plug in L1 or L2 loss \n",
    "* Plug in the model so that the resulting expression is a function of $\\theta$\n",
    "\n",
    "Then, find the minimum of the objective function\n",
    "1) Differntiate with respect to $\\theta$\n",
    "2) Set equal to $0$\n",
    "3) Solve for $\\hat{\\theta}$\n",
    "4) (If we have multiple parameters) repeat 1-3 with partial derivatives\n",
    "\n",
    "MSE has a property - **convexity** - tha guarantees $R(\\hat{\\theta})$ is a global minimum\n",
    "* MAE is also convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparing Loss Functions** ####\n",
    "\n",
    "Shape\n",
    "* The loss surface of MSE is smooth - easy to minimize\n",
    "* The loss surface of MAE is Piecewise\n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/constant_model_loss_transformations/images/mse_loss_26.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/constant_model_loss_transformations/images/mae_loss_infinite.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "Outliers: \n",
    "\n",
    "* Under MSE, the optimal parameter $\\hat{\\theta}$ is strongly affected by the presence out outliers \n",
    "* Under MAE, the optimal parameters is not as influenced by outlying data \n",
    "\n",
    "So, MSE **sensitive** to outliers, while the MAE is **robust** to outliers \n",
    "\n",
    "Single optimal value\n",
    "* MSE has a **unique** solution for $\\hat{\\theta}$ \n",
    "\n",
    "* MAE is not guaranteed to have a single unique solution (when the number of observations in $y$ is even then there could be multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformations to fit Linear Models**\n",
    "\n",
    "Even if we make transformation to specific features, as long as our parameters are still **linear**, $\\theta = [\\theta_0, \\theta_1]$ then all of our ideas above can still be applied\n",
    "\n",
    "* In terms of specific transformations to use, refer to the Bulge Diagram from the Visualization Notes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
