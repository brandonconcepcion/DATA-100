{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ordinary Least Squares**\n",
    "\n",
    "### **Multiple Linear Regression**\n",
    "\n",
    "MLS is an extension of Simple Linear Regression (SLR) in that it uses multiple features in our model  \n",
    "\n",
    "From discussion: \n",
    "* If $\\hat{\\theta_1} = \\frac{r \\sigma_{speed}}{\\sigma_{turn}}$\n",
    "* If from a different model $\\hat{\\theta_w,1} =  \\frac{r \\sigma_{turn}}{\\sigma_{speed}}$\n",
    "* To go from $\\hat{\\theta_w,1}$ to $\\hat{\\theta_1}$, we multiply by $\\frac{\\sigma_{speed}^2}{\\sigma_{turn} ^2}$ in order to get our stuff in the right units\n",
    "\n",
    "\n",
    "The multiple linear regression model takes on the form: \n",
    "\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_p x_p $$\n",
    "\n",
    "* Our predicted value of $y$, $\\hat{y}$ is a linear combination of the single **observations** (features), $x_i$, and parameters $\\theta_i$\n",
    "\n",
    "We defint the **parameter vector** as the vector of all of our \"weights\" for each specific feature \n",
    "* We often incorporate our bias term here as well \n",
    "\n",
    "$$\\theta = \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{p}\n",
    "         \\end{bmatrix}$$\n",
    "\n",
    "We work with two vectors in OLS: \n",
    "1) A row vector corresponding to our observed data\n",
    "2) A column vector containing the model parameters\n",
    "\n",
    "This is equivalent to the **dot (scalar)** product of the observation vector and parameter vector \n",
    "\n",
    "$$[1,\\:x_{1},\\:x_{2},\\:x_{3},\\:...,\\:x_{p}] \\theta = [1,\\:x_{1},\\:x_{2},\\:x_{3},\\:...,\\:x_{p}] \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{p}\n",
    "         \\end{bmatrix} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:...\\:+\\:\\theta_p x_{p}$$\n",
    "\n",
    "Notice that we have inserted $1$ as the first value in the obseration vector \n",
    "* When the dot product is computed, this $1$ will be multiplied with $\\theta_0$ to give the intercept of the regression model \n",
    "* We call this $1$ entry the **intercept** or **bias** term \n",
    "\n",
    "In statistics, this kind of model + loss is called **Ordinary Least Squares (OLS)**\n",
    "* The solution to OLS is the minimizing loss for parameters $\\hat{\\theta}$, also called the **least squares estimate**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some of the Linear Algebra**\n",
    "\n",
    "* We can express our linear regression in terms of Matrices \n",
    "\n",
    "$$ \\hat{Y} = X\\Theta$$\n",
    "\n",
    "* Here, $\\hat{Y}$ is the **prediction vector** with $n$ elements, and contains the prediction made by the model for each one of our $n$ observations \n",
    "* $X$ is the **design matrix** \n",
    "* $\\Theta$ is the **parameter vector** \n",
    "* Note that our **true output** $Y$ is also a vector with $n$ elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss Functions**\n",
    "\n",
    "* For OLS, we define MSE (L2 loss) as the following: \n",
    "\n",
    "$$R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} (||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2)^2$$\n",
    "\n",
    "or \n",
    "\n",
    "$$R(\\theta) = \\frac{1}{n} (||\\mathbb{Y} - \\mathbb{X} \\theta||_2)^2$$\n",
    "\n",
    "So, our new task is to fit the optimal parameter vector $\\Theta$ such that the cost function is minimized \n",
    "\n",
    "* We want to **minimize** the distance between the vectors of true values $Y$, and the predicted values $\\hat{Y}$\n",
    "* We want to minimize the **length** of the **residual** vector, defined as \n",
    "\n",
    "$$e = \\mathbb{Y} - \\mathbb{\\hat{Y}} = \\begin{bmatrix}\n",
    "         y_1 - \\hat{y}_1 \\\\\n",
    "         y_2 - \\hat{y}_2 \\\\\n",
    "         \\vdots \\\\\n",
    "         y_n - \\hat{y}_n\n",
    "       \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Terminology for MLS**\n",
    "\n",
    "* $x$ is most often called \n",
    "    * Feature(s)\n",
    "    * Independent variable(s)\n",
    "\n",
    "* $y$ can be called an \n",
    "    * Output\n",
    "    * Response \n",
    "\n",
    "* $\\hat{y}$ can be called a\n",
    "    * Prediction \n",
    "\n",
    "* $\\theta$ can be called a\n",
    "    * Weight(s)\n",
    "    * Parameter(s)\n",
    "\n",
    "* $\\hat{\\theta}$ can be called a\n",
    "    * Estimator(s)\n",
    "    * Optimal parameters \n",
    "\n",
    "A datapoint $(x,y)$ is als called an observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some of the Geometry**\n",
    "\n",
    "* We can think of $\\hat{Y}$ as a **linear combination of feature vectors**, scaled by the **parameters**\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/ols/images/columns.png\" alt=\"Image Alt Text\" width=\"400\" height=\"200\">\n",
    "\n",
    "$$\\hat{\\mathbb{Y}} =\n",
    "\\theta_0 \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           1\n",
    "         \\end{bmatrix} + \\theta_1 \\begin{bmatrix}\n",
    "           x_{11} \\\\\n",
    "           x_{21} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n1}\n",
    "         \\end{bmatrix} + \\ldots + \\theta_p \\begin{bmatrix}\n",
    "           x_{1p} \\\\\n",
    "           x_{2p} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{np}\n",
    "         \\end{bmatrix}\n",
    "         = \\theta_0 \\mathbb{X}_{:,\\:1} + \\theta_1 \\mathbb{X}_{:,\\:2} + \\ldots + \\theta_p \\mathbb{X}_{:,\\:p+1}$$\n",
    "\n",
    "* The sum of residuals is not always equal to $0$, but in general it is\n",
    "  * In the design matrix $X$, all observations must have the same intercept term in order for the sum of the residuals to be $0$\n",
    "\n",
    "Since $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$ is a **linear combination** of the columns of $X$, we know that the **predictions are contained in the span** of $X$\n",
    "* So we know that $\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})$\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/ols/images/span.png\" alt=\"Image Alt Text\" width=\"400\" height=\"200\">\n",
    "\n",
    "* Remember that the modeling fitting goal is to generate predictions such that the distance between the true vector of true values, $Y$, and the vector od predicted $\\hat{Y}$ is minimized \n",
    "* This means **we want $\\hat{Y}$ to be the vector in $\\text{Span}(X)$ that is closest to $Y$**\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/ols/images/residual.png\" alt=\"Image Alt Text\" width=\"400\" height=\"200\">\n",
    "\n",
    "The vector in $\\text{Span}(X)$ that is closest to $Y$ is always the **orthogonal projection** of $Y$ onto $\\text{Span}(X)$\n",
    "* Thus, we choose $\\theta$ that makes the **residual vector orthogonal to any vector in** $\\text{Span}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Least Squares Estimate**\n",
    "\n",
    "Any vector $\\theta$ that minimizes MSE on a dataset must satisfy the following equation \n",
    "\n",
    "$$\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "We call this the **least squares estimate** of $\\Theta$, and is only valid if $\\mathbb{X}^T \\mathbb{X}$ is invertible \n",
    "* This means that $\\mathbb{X}^T \\mathbb{X}$ needs to be full column rank, which happens when $X$ is full column rank\n",
    "    * This does NOT mean that $X$, $X^T$, nor $Y$ need to be invertible\n",
    "\n",
    "* This also means that we cannot derive solutions for anything with l1 loss, since the normal equation optimizes for MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Root Mean Squared Error**\n",
    "\n",
    "* Is very similar to MSE, but now we just take the square root!\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "* A low RMSE indicates more \"accurate\" predictions - that there is lower average loss acoss the dataset\n",
    "\n",
    "* Taking the square root like this also converts our value back into original, non-squared units of $y_i$, which is useful for understanding model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multiple** $R^2$\n",
    "\n",
    "* **Multiple** $R^2$ is also referred to as the **coefficient of determination**, as is the proportion of variance of our **fitted values** $\\hat{y}_i$ to our true values of $y$\n",
    "* Ranges from $0$ to $1$ and is effecitvely the *proportion* of variance in the observations that the **model explains**\n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of } \\hat{y}_i}{\\text{variance of } y_i} = \\frac{\\sigma^2_{\\hat{y}}}{\\sigma^2_y}$$\n",
    "\n",
    "* We can interpret $R^2$ as being the correlation between $y$ and $\\hat{y}$ (for OLS with an intercept term)\n",
    "* Typically, as we add more features $R^2$ also goes up (not always a good thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Properties of OLS**\n",
    "\n",
    "1) When using the optimal parameter vector, our residuals $e = Y = \\hat{Y}$ are orthogonal to $span(\\mathbb{X})$\n",
    "* $\\mathbb{X}^Te = 0$\n",
    "\n",
    "2) For all linear models with an **intercept term**, the **sum of residuals is zero**\n",
    "* $\\sum_i^n e_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
