{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n",
    "\n",
    "One major caveat for our linear models up to this point: They've only worked with linear input variables \n",
    "* In other words, we've assumed that our response / predictions could be a linear combination of linear variables \n",
    "\n",
    "We define feature engineering as the process of *transforming* raw features into *more informative features* that can be used in modeling or EDA tasks and improve model performace \n",
    "\n",
    "### **Feature Functions**\n",
    "* A feature function descrbes the transformations we apply to raw features in a dataset in order to create a design matrix of transformed features \n",
    "\n",
    "We denote this feature function as $\\Phi$\n",
    "* So $\\Phi(\\mathbb{X})$ is the transformed design matrix ready to be used in modeling \n",
    "\n",
    "Here is an example: \n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/phi.png\" alt=\"Image Alt Text\" width=\"700\" height=\"320\">\n",
    "\n",
    "The new features introduced by $\\Phi$ can then be used in modeling \n",
    "* we often use $\\phi_i$ to represent transformed features after feature engineering \n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{y} &= \\theta_0 + \\theta_1 x + \\theta_2 x^2 \\\\\n",
    "\\hat{y} &= \\theta_0 + \\theta_1 \\phi_1 + \\theta_2 \\phi_2\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One Hot Encoding**\n",
    "\n",
    "How do we quantify *non-numeric* features? We can make use of **one hot encoding**, which generates numeric features from categorical data, allowing us to use our usual methods to fit a regression model on the data \n",
    "\n",
    "For example, take a `\"day\"` column in a table. \n",
    "* Each entry correponds to the day of the week, such as Sunday, Monday, etc. \n",
    "* For each unique entry in `\"day\"`, we fill the corresponding feature in the new table with a $1$, the rest $0$\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/ohe.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "So each category of a categorical variable gets its own feature \n",
    "* Value = $1$ if a row belongs to a category \n",
    "* Value = $0$ otherwise \n",
    "\n",
    "One thing to keep in mind about one-hot encoded columns is that the sum of all the one-hot encoded columns will sum to $1$, which represents the bias column \n",
    "* The bias column is a linear combination of the one hot encoded columns \n",
    "\n",
    "There are two options to remedy this: \n",
    "1) We can omit a feature column \n",
    "2) We can get rid of our bias column \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/remove.png\" alt=\"Image Alt Text\" width=\"700\" height=\"160\">\n",
    "\n",
    "Had we not done anything, our OLS estimate $\\hat{\\theta} = (\\mathbb{X}^{\\top}\\mathbb{X})^{-1}\\mathbb{X}^{\\top}\\mathbb{Y}$ would fail "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Polynomial Features**\n",
    "\n",
    "Consider the following scatterplot: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/feature_engineering_files/figure-html/cell-5-output-2.png\" alt=\"Image Alt Text\" width=\"500\" height=\"350\">\n",
    "\n",
    "We can see that the data follows a curved line rather than a straight line, indicating a **non-linear** relationship between the features \n",
    "* One potential remedy is to introduce a **polynomial** term, such that we have: \n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp}^2)$$\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 \\phi_1 + \\theta_2 \\phi_2$$\n",
    "\n",
    "How can we fit a model with non-linear features?\n",
    "* We can still the same thing! Our model is still technically **linear**\n",
    "* Although it contains non-linear features, it is linear with respect to the model *parameters* \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/feature_engineering_files/figure-html/cell-6-output-2.png\" alt=\"Image Alt Text\" width=\"500\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Complexity and Overfitting** \n",
    "\n",
    "* Feature engineering allows us to build all sorts of featrues to improve our feature, substantially allowing us to capture the model's ability to capture non-linear relationships \n",
    "\n",
    "What happens if we keep adding features to our model?\n",
    "\n",
    "Let's plot models as complexity increases from $0$ to $7$ and observe the RMSE: \n",
    "\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/degree_comparison2.png\" alt=\"Image Alt Text\" width=\"700\" height=\"280\">\n",
    "\n",
    "* As we use our model to make predictions on the same data that was used to fit the model, we find the MSE decreases with each additional polynomial \n",
    "\n",
    "* The **training error**, or model's error evaluated on the training set, seems to go down as complexity increases \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/train_error.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "One huge drawback of creating a complicated model is that if we make it *too* complex, we'll start to overfit to our training data \n",
    "* **overfitting** is the phenomenon where the model starts to \"memorize\" our training data, leaving it unable to **generalize** well to data it's never seen before. \n",
    "\n",
    "We say that complex models have high **variance**, as they tend to vary dramatically for small changes in different data sets \n",
    "\n",
    "There's now this dilemma: We can go about *decreasing* training error with increasing model complexity, but models that are *too* complex start to overfit and can't be reapplied to new datasets due to **high variance**\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/feature_engineering/images/bvt.png\" alt=\"Image Alt Text\" width=\"500\" height=\"300\">\n",
    "\n",
    "\n",
    "The key point here: We need to strike a balance in the complexity of our models; we want models that are capable of \"generalizing\" to unseen data \n",
    "* A model too simple won't capture the true relationships between our variable of interest \n",
    "* A model too complex runs the risk of overfitting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
