{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **16 Cross Validation and Regularization**\n",
    "\n",
    "The balance of complexity: \n",
    "* A model that is too complex can lead to overfitting \n",
    "* A model that is too simple can lead to underfitting \n",
    "\n",
    "How do we control model complexity to avoid under - and - overfitting?\n",
    "\n",
    "We can make use of **cross-validation** in order to assess *when* our model begins to overfit \n",
    "\n",
    "We can also apply **regularization** in order to adjust the complexity of our models ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16.1 Cross Validation and Regularization**\n",
    "\n",
    "#### **16.1.1 Training, Test, and Validation Sets**\n",
    "\n",
    "* We know that *increasing* model complexity *decreased* our model's training error, but *increased* its variance \n",
    "\n",
    "##### **16.1.1.1 Test Sets**\n",
    "\n",
    "* In order to assess our model on \"unseen\" data, we can make use of a **test set**\n",
    "* The datapoints in this set will *not* be used to fit the model \n",
    "\n",
    "* After, we use the remaining portion of our data - which we call the **training set** - to run OLS, gradient descent, or something else \n",
    "* After we construct our model on the training data, we assess its performance on the test set (this is indicative of how well it can make predictions on *unseen* data)\n",
    "\n",
    "* We can only use the test set **once**: to compute the performance of the model after all fine-tuning has been completed \n",
    "\n",
    "This process of dividing our data is known as **train - test - split**\n",
    "* Usually we split it up so we have $10\\%$ or $20\\%$ as our test \n",
    "\n",
    "\n",
    "<img src=\"\n",
    "https://ds100.org/course-notes/cv_regularization/images/train-test-split.png\" alt=\"Image Alt Text\" width=\"500\" height=\"170\">\n",
    "\n",
    "##### **16.1.1.2 Validation Sets**\n",
    "\n",
    "What if we were dissatisfied with our test set's performance?\n",
    "* We can't go back and adjust our model, as that would no longer be a true representation of the model's performance on *unseen* data \n",
    "\n",
    "The solution? \n",
    "* Introduce a **validation set**\n",
    "* A validation set is a random portion of the *training set* that is set aside for assessing model performance while the model is *still being developed* \n",
    "\n",
    "1) Perform a train-test split \n",
    "2) Set the test aside; we will not touch it until the end of the model design process \n",
    "3) Set aside a portion of the training to be used for validation \n",
    "4) Fir model parameters to the datapoints in the remaining training \n",
    "5) Assess model performance on validation, and adjust the model as needed. Re-fit model to the remaining portion of the training, the reevaluate on the validation set until you are satisfied \n",
    "6) After *all* the model development is complete, assess the model's performance on the test set \n",
    "\n",
    "The process of creating a validation set is called a **validation split**\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/cv_regularization/images/validation-split.png\" alt=\"Image Alt Text\" width=\"600\" height=\"170\">\n",
    "\n",
    "Validation error decreases *then increases* as we increase model complexity\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/cv_regularization/images/training_validation_curve.png\" alt=\"Image Alt Text\" width=\"600\" height=\"460\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross Validation**\n",
    "* Cross validation subdivides the training set into two groups; one group used to fit the model (a mini training set), and the other used to validatie it (a validation set)\n",
    "\n",
    "* $k$-fold cross-validation means that we perform this splitting step $k$ times. Each validation set contains \\frac{1}{k} of the total training data \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/cv_regularization/images/model_selection.png\" alt=\"Image Alt Text\" width=\"600\" height=\"240\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regularization**\n",
    "\n",
    "* One way or limiting complexity is saying that our model parameters can't be too large \n",
    "* In regularization, we penalize our model for picking large theta values y modifying our loss function \n",
    "\n",
    "$$\\frac{1}{n} \\Sigma_{i = 1}^{n} \\text{Loss}(y_i, \\hat{y_i}) + \\lambda R(\\theta)$$\n",
    "\n",
    "* There is now an extra \"cost\" to choosing large values of theta. We won't choose the best possible theta vector (higher model bias), but we'll avoid overfitting (lower model variance)\n",
    "\n",
    "* What does $R(\\theta)$ look like?\n",
    "* In Data 100, we'll mostly consider two types of regularized regression \n",
    "\n",
    "**LASSO(L1) Regression:**\n",
    "\n",
    "$$ \\frac{1}{n} |Y - X\\theta|_{2}^{2} + \\lambda \\cdot \\Sigma_{j = 1}^{d} |\\theta_j|$$\n",
    "\n",
    "* Encourage sparsity (drive some values of $\\theta$ to 0)\n",
    "* No closed - form solution for the optimal $\\theta$\n",
    "* The penalty term added to the loss function is the sum of the absolute values of the coefficients\n",
    "\n",
    "**Ridge(L2) Regression:**\n",
    "\n",
    "$$ \\frac{1}{n} |Y - X\\theta|_{2}^{2} + \\lambda \\cdot \\Sigma_{j = 1}^{d} \\theta_j^2$$\n",
    "\n",
    "* \"Robust\" - tends to spread theta weights over many features \n",
    "* Optimal $\\theta$ can be found: \n",
    "    * $\\hat{\\theta} = (X^T X + n \\lambda I)^{-1} X^TY$\n",
    "* The penalty term added to the loss function is the sum of the squares of the coefficients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
