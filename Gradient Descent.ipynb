{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Descent**\n",
    "\n",
    "Up to this point,the techniques we have used to find the optimal parameter values could only have been applied if we made some big assumptions \n",
    "\n",
    "* When we took the partial derivative with respect to $\\theta$, we assumed the loss function was differentiable at all points, where we could algebracially solve for the zero points of the derivative \n",
    "\n",
    "* For the geometric approach, $\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1}\\mathbb{X}^T \\mathbb{Y}$, OLS *only* applies when using a linear model with MSE loss \n",
    "\n",
    "In the case we have more complex models with more complex loss functions, we need to make use of a new technique called **gradient descent**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Minimizing an Arbitrary 1D Function**\n",
    "\n",
    "Suppose we have the loss function as below, and we want to find the specific $x$ value which minimizes this function: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/arbitrary.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "A naive approach would involve finding the error for *each* $x$ value, or only selecting a certain sample of points to use. \n",
    "* Either way, both of these methods are prone to error as well as are computationally inefficient \n",
    "\n",
    "### **Digging into Gradient Descent**\n",
    "\n",
    "Looking at the graph above, it's clear the minimum value occurs at $\\theta = 5.3$. \n",
    "\n",
    "* In most cases, we won't be able to *see* this value, so let's pretend for a second that we don't see it either. \n",
    "\n",
    "The first derivative of this loss function can give us a clue on its minimum point \n",
    "\n",
    "Let's pick an arbitrary $\\theta$ value\n",
    "* If we pick a $\\theta$ that is smaller than our $\\hat{\\theta}$, then we undershoot the true minimum, and the derivative will be **negative**\n",
    "    * If we move further to the right, we can *decrease* our loss function further\n",
    "\n",
    "* If we pick a $\\theta$ that is larger than our $\\hat{\\theta}$, then we overshoot the true minimum, and the derivative will be **positive**\n",
    "    * If we move further to the left, we can *decrease* our loss function further\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/step.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "We can use this pattern to help formulate our next guess for the optimal $\\hat{\\theta}$\n",
    "\n",
    "Consider the case we undershot $\\hat{\\theta}$ by guessing too low of a value. We'll want our next guess to be greater than our previous guess - we want to shift our guess to the right \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/neg_step.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "* You can think of this as following the slope \"downhill\" to the minimum value \n",
    "\n",
    "Consider the other case: Where we overshot $\\hat{\\theta}$ by guessing too *high* of a value. We'll want our next guess to be lower in value than our previous guess - we want to shift our guess to the left\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/pos_step.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "We've seen that the derivative of the function at each point tells use the direction of our next guess: \n",
    "\n",
    "* A negative slope means we want to step to the right, or move in the *positive* direction \n",
    "* A positive slope means we want to step to the left, or move in the *negative* direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the intuition from above, we know that for the next point we choose, we will want to move in the *negative direction of the slope*\n",
    "\n",
    "We also notice that *had* we started at a point that was all the way to the left, we would have eventually converged at the local minimum\n",
    "\n",
    "* This is an issue with our loss function, and we describe this kind of loss function as being *non-convex*\n",
    "\n",
    "* This would cause problems with gradient descent, but in DATA 100 we assume that we will only be using convex loss functions such as MSE \n",
    "\n",
    "We define a convex function in simple terms: \n",
    "* If you drew a line between any two points on the curve, all values on the curve must be on or below the line.\n",
    "\n",
    "* Any local minimum of a convex function is also its global minimum so we avoid the situation where the algorithm converges on some critical point that is not the minimum of the function.\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/convex.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "In essence, gradient descent is a powerful technique for selecting the specific values of $\\theta_i$ which will minimize the loss function on the data \n",
    "\n",
    "when using gradient descent in a modeling context, we: \n",
    "1) Make a guess for minimizing $\\theta_i$ \n",
    "2) Compute the derivative of the loss functon $L$\n",
    "\n",
    "In the process of jumping between points in order to find the minimum values of $\\theta$, we defined finding the value(s) of theta as: \n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{d}{d\\theta} L(\\theta^{(t)})$$\n",
    "\n",
    "Here is what each of the values represent: \n",
    "* $\\theta^{(t+1)}$: The updated parameters at the next iteration, or at time $t + 1$\n",
    "\n",
    "* $\\theta^{(t)}$: The current parameter at the current iteration, or at current time $t$\n",
    "\n",
    "* $\\alpha$: This is the learning rate, which is a hyperparameter that controls the step size of each update \n",
    "    * A smaller learning rate means we take smaller steps\n",
    "    * A larger learning rate means we take larger steps \n",
    "\n",
    "* $\\frac{d}{d\\theta} L(\\theta^{(t)})$: Represents the derivative of the loss function at the current parameter value $\\theta^{(t)}$\n",
    "    * We often have to calculate this ourselves, on an exam problem perhaps, when want to use it in our algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this process works as below: \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/gradient_descent_files/figure-html/cell-21-output-2.png\" alt=\"Image Alt Text\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Descent on Multi-Dimensional Models**\n",
    "\n",
    "Now suppose that we have a cost function with multiple parameters \n",
    "\n",
    "Consider a simple linear regression model with $2$ parameters: \n",
    "\n",
    "$$\\hat{y} + \\theta_0 + \\theta_1x$$\n",
    "\n",
    "Also consider a multiple linear regression model with $p+1$ parameters: \n",
    "\n",
    "$$\\mathbb{Y} = \\theta_0 + \\theta_1 \\Bbb{X}_{:,1} + \\theta_2 \\Bbb{X}_{:,2} + \\cdots + \\theta_p \\Bbb{X}_{:,p}$$\n",
    "\n",
    "We'll need to expand gradient descent so we can update our guesses for all model parameters in one go \n",
    "\n",
    "With multiple parameters to optimize, we now consider a **loss surface**, or the model's loss for a particular *combination* of possible parameters \n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/loss_surface.png\" alt=\"Image Alt Text\" width=\"700\" height=\"500\">\n",
    "\n",
    "As before, the derivative of the loss function tells us the best way towards the minimum value \n",
    "* On a 2D surface of higher, the best way to go down (gradient) is described by a *vector*\n",
    "\n",
    "For the *vector* of parameter values $\\vec{\\theta} = \\begin{bmatrix}  \\theta_{0} \\\\  \\theta_{1} \\\\  \\end{bmatrix}$, we take the *partial derivative* of each loss with respect to each parameter $\\frac{\\partial L}{\\partial \\theta_0}$ and $\\frac{\\partial L}{\\partial \\theta_1}$\n",
    "\n",
    "The **gradient vector** of a generic function of $p+1$ variables is therefore \n",
    "\n",
    "$$\\nabla_{\\vec{\\theta}} L =  \\begin{bmatrix} \\frac{\\partial L}{\\partial \\theta_0} \\\\ \\frac{\\partial L}{\\partial \\theta_1} \\\\ \\vdots \\end{bmatrix}$$\n",
    "\n",
    "Where $\\nabla_\\theta L$ always points in the downhill direction of the surface\n",
    "* We can interpret this as \"if I nudge the $i$ th model weight, what happens to the loss?\"\n",
    "\n",
    "We can use this idea of the gradient to now update our previous 1D gradient rule in order to accomodate models with multiple parameters \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "         \\theta_{0}^{(t+1)} \\\\\n",
    "         \\theta_{1}^{(t+1)} \\\\\n",
    "         \\vdots\n",
    "       \\end{bmatrix} = \\begin{bmatrix}\n",
    "         \\theta_{0}^{(t)} \\\\\n",
    "         \\theta_{1}^{(t)} \\\\\n",
    "         \\vdots\n",
    "       \\end{bmatrix} - \\alpha \\begin{bmatrix}\n",
    "         \\frac{\\partial L}{\\partial \\theta_{0}} \\\\\n",
    "         \\frac{\\partial L}{\\partial \\theta_{1}} \\\\\n",
    "         \\vdots \\\\\n",
    "       \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In a more compact form, this looks like: \n",
    "\n",
    "$$\\vec{\\theta}^{(t+1)} = \\vec{\\theta}^{(t)} - \\alpha \\nabla_{\\vec{\\theta}} L(\\theta^{(t)})$$\n",
    "\n",
    "In this new form, this is what each one of the values represents: \n",
    "* $\\theta$ is a vector with our model weights\n",
    "* $L$ is the loss function \n",
    "* $\\alpha$ is the learning rate (ours is constant, but other techniques use an $\\alpha$ which decreases over time)\n",
    "* $\\vec{\\theta}^{(t)}$ is the curret value of $\\theta$\n",
    "* $\\vec{\\theta}^{(t+1)}$ is the next value of $\\theta$\n",
    "* $\\nabla_{\\vec{\\theta}} L(\\theta^{(t)})$ is the gradient of the loss function evaluated at the current $\\vec{\\theta}^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Gradient Descent and Stochastic Gradient Descent**\n",
    "\n",
    "In more formal terms, we just derived what is called **batch gradient descent**\n",
    "* For each iteration of the algorithm, the derivative of loss is computed across the *entire batch* of all $n$ datapoints\n",
    "* Not great for super large datasets, as finding the gradient across all the data is incredibly computationally taxing \n",
    "\n",
    "We now define **Stochastic (mini-batch) gradient descent**, which uses only a *sample* of the full dataset in each update \n",
    "* We estimate the true gradient of the loss surface using only a sample of our data \n",
    "* The **batch size** is the number of data points we use in each sample, where the sampling strategy is generally used without replacement\n",
    "\n",
    "Each complete \"pass\" through the data is known as a **training epoch** \n",
    "\n",
    "After shuffling the data, in a single **training epoch** of gradient descent, we \n",
    "* Compute the gradient on the first $x\\%$ of the data. Update the parameter guesses \n",
    "* Compute the gradient on the next $x\\%$ of the data. Update the parameter guesses \n",
    "* Compute the gradient on the last $x\\%$ of the data. Update the parameter guesses\n",
    "\n",
    "Every data point appears only once in a single training epoch. We perforom several traning epochs until we're satisfied \n",
    "\n",
    "Batch size gradient descent will always advance towards the minimum of our loss surface \n",
    "\n",
    "By constrast, stochastic gradient descent has a degree of randomness, as only a subset of the full data is used to update the guess for $\\vec{\\theta}$ at each iteration\n",
    "* In the long term, these stochastic techniques should converge to the optimal solution\n",
    "\n",
    "<img src=\"https://ds100.org/course-notes/gradient_descent/images/stochastic.png\" alt=\"Image Alt Text\" width=\"700\" height=\"300\">\n",
    "\n",
    "* We can see that with batch gradient descent, there's a visible path towards the optimal $\\hat{\\theta}$\n",
    "\n",
    "* Stochastic gradient descent, in contrast, \"hops around\" on its path to the minimim point on the loss surface, reflecting the randomness of the sampling process at each step\n",
    "\n",
    "To summarize the tradeoffs of batch size:\n",
    "\n",
    "Pros\n",
    "* Smaller batch size: More frequent gradient updates \n",
    "* Larger batch size: Leverage hardward acceleration to improve overall system performance and higher quality gradient updates \n",
    "\n",
    "Cons \n",
    "* Smaller batch size: More variability in gradient estimates\n",
    "* Larger batch size: Less frequent gradient updates"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
